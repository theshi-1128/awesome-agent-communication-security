# Risks from L3
**Note**: Due to the forward-looking nature of this paper, the risks we mentioned include both **the proposed ones** and **potential directions that have not been studied**. As a result, some risks may lack a concrete list of papers, we hope our paper can inspire these directions.
## R1: Text-Based Attacks
- [2022/11] **[Ignore previous prompt: Attack techniques for language models](https://arxiv.org/abs/2211.09527)**
- [2023/02] **[Not what you're signed up for: Compromising real-world llm-integrated applications with indirect prompt injection](https://arxiv.org/abs/2302.12173)**
- [2023/05] **[Jailbreaking chatgpt via prompt engineering: An empirical study](https://arxiv.org/abs/2305.13860)**
- [2023/06] **[Prompt Injection attack against LLM-integrated Applications](https://arxiv.org/abs/2306.05499)**
- [2023/07] **[Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models](https://arxiv.org/abs/2307.14539)**
- [2023/07] **[MASTERKEY: Automated jailbreaking of large language model chatbots](https://arxiv.org/abs/2307.08715)**
- [2023/08] **["Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models](https://arxiv.org/abs/2308.03825)**
- [2023/10] **[Autodan: Generating stealthy jailbreak prompts on aligned large language models](https://arxiv.org/abs/2310.04451)**
- [2023/10] **[Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/abs/2310.08419)**
- [2023/10] **[Formalizing and benchmarking prompt injection attacks and defenses](https://arxiv.org/abs/2310.12815)**
- [2023/11] **[A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](https://arxiv.org/abs/2311.08268)**
- [2023/11] **[DeepInception: Hypnotize Large Language Model to Be Jailbreaker](https://arxiv.org/abs/2311.03191)**
- [2024/02] **[CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models](https://arxiv.org/abs/2402.16717)**
- [2024/02] **[Making them ask and answer: Jailbreaking large language models in few queries via disguise and reconstruction](https://arxiv.org/abs/2402.18104)**
- [2024/02] **[Poisonedrag: Knowledge corruption attacks to retrieval-augmented generation of large language models](https://arxiv.org/abs/2402.07867)**
- [2024/02] **[Pandora: Jailbreak gpts by retrieval augmented generation poisoning](https://arxiv.org/abs/2402.08416)**
- [2024/03] **[Automatic and universal prompt injection attacks against large language models](https://arxiv.org/abs/2403.04957)**
- [2024/03] **[Here comes the AI worm: Unleashing zero-click worms that target GenAI-powered applications](https://arxiv.org/abs/2403.02817)**
- [2024/03] **[Optimization-based prompt injection attack to llm-as-a-judge](https://arxiv.org/abs/2403.17710)**
- [2024/04] **[Many-shot jailbreaking](https://www.anthropic.com/research/many-shot-jailbreaking)**
- [2024/05] **[Phantom: General trigger attacks on retrieval augmented language generation](https://arxiv.org/abs/2405.20485)**
- [2024/07] **[Breaking agents: Compromising autonomous llm agents through malfunction amplification](https://arxiv.org/abs/2407.20859)**
- [2024/07] **[Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models](https://arxiv.org/abs/2407.16205)**
- [2024/08] **[{LLM-Fuzzer}: Scaling Assessment of Large Language Model Jailbreaks](https://www.usenix.org/conference/usenixsecurity24/presentation/yu-jiahao)**
- [2024/10] **[Backdoored Retrievers for Prompt Injection Attacks on Retrieval Augmented Generation of Large Language Models](https://arxiv.org/abs/2410.14479)**
- [2024/10] **[Denial-of-service poisoning attacks against large language models](https://arxiv.org/abs/2410.10760)**
- [2024/10] **[Safeguard is a Double-edged Sword: Denial-of-service Attack on Large Language Models](https://arxiv.org/abs/2410.02916)**
- [2024/12] **[Crabs: Consuming Resrouce via Auto-generation for LLM-DoS Attack under Black-box Settings](https://arxiv.org/abs/2412.13879)**
- [2025/02] **[OVERTHINKING: Slowdown Attacks on Reasoning LLMs](https://arxiv.org/abs/2502.02542)**
- [2025/02] **[Why are web ai agents more vulnerable than standalone llms? a security analysis](https://arxiv.org/abs/2502.20383)**
- [2025/04] **[Wasp: Benchmarking web agent security against prompt injection attacks](https://arxiv.org/abs/2504.18575)**
- [2025/04] **[Rag llms are not safer: A safety analysis of retrieval-augmented generation for large language models](https://arxiv.org/abs/2504.18041)**
- [2025/05] **[Ip leakage attacks targeting llm-based multi-agent systems](https://arxiv.org/abs/2505.12442)**
- [2025/09] **[Web Fraud Attacks Against LLM-Driven Multi-Agent Systems](https://arxiv.org/abs/2509.01211)**

## R2: Multimodal Attacks
- [2023/07] **[Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models](https://arxiv.org/abs/2307.14539)**
- [2023/11] **[Figstep: Jailbreaking large vision-language models via typographic visual prompts](https://arxiv.org/abs/2311.05608)**
- [2024/02] **[CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models](https://arxiv.org/abs/2402.16717)**
- [2024/05] **[Visual-roleplay: Universal jailbreak attack on multimodal large language models via role-playing image character](https://arxiv.org/abs/2405.20773)**
- [2024/05] **[White-box multimodal jailbreaks against large vision-language models](https://arxiv.org/abs/2405.17894)**
- [2024/05] **[Voice jailbreak attacks against gpt-4o](https://arxiv.org/abs/2405.19103)**
- [2024/06] **[Jailbreak vision language models via bi-modal adversarial prompt](https://arxiv.org/abs/2406.04031)**
- [2024/07] **[Breaking agents: Compromising autonomous llm agents through malfunction amplification](https://arxiv.org/abs/2407.20859)**
- [2024/11] **[Jailbreak Large Visual Language Models Through Multi-Modal Linkage](https://arxiv.org/abs/2412.00473)**
- [2024/11] **[Transferable adversarial attacks against asr](https://arxiv.org/abs/2411.09220)**
- [2024/11] **[Exploring Visual Vulnerabilities via Multi-Loss Adversarial Search for Jailbreaking Vision-Language Models](https://arxiv.org/abs/2411.18000)**
- [2024/12] **[AdvWave: Stealthy Adversarial Jailbreak Attack against Large Audio-Language Models](https://arxiv.org/abs/2412.08608)**
- [2025/02] **[Distraction is all you need for multimodal large language model jailbreaking](https://arxiv.org/abs/2502.10794)**
- [2025/04] **[Multilingual and multi-accent jailbreaking of audio llms](https://arxiv.org/abs/2504.01094)**
- [2025/06] **[Pushing the Limits of Safety: A Technical Report on the ATLAS Challenge 2025](https://arxiv.org/abs/2506.12430)**

## R3: Violation of User Privacy
- [2024/09] **[Eia: Environmental injection attack on generalist web agents for privacy leakage](https://arxiv.org/abs/2409.11295)**
- [2025/02] **[Unveiling privacy risks in llm agent memory](https://arxiv.org/abs/2502.13172)**
- [2025/03] **[Multi-agent systems execute arbitrary malicious code](https://arxiv.org/abs/2503.12188)**
- [2025/05] **[PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization](https://arxiv.org/abs/2505.09921)**
- [2025/05] **[Ip leakage attacks targeting llm-based multi-agent systems](https://arxiv.org/abs/2505.12442)**
- [2025/05] **[Automated Profile Inference with Language Model Agents](https://arxiv.org/abs/2505.12402)**
- [2025/06] **[Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution](https://arxiv.org/abs/2506.01055)**
- [2025/06] **[Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers](https://arxiv.org/abs/2506.15674)**

## R4: Psychological and Social Manipulation
- [2023/08] **[AI deception: A survey of examples, risks, and potential solutions](https://arxiv.org/abs/2308.14752)**
- [2024/10] **[Prompt infection: Llm-to-llm prompt injection within multi-agent systems](https://arxiv.org/abs/2410.07283)**

## R5: Execution of Malicious and Harmful Tasks
- [2021/08] **[Asleep at the keyboard? assessing the security of github copilot's code contributions](https://arxiv.org/abs/2108.09293)**
- [2023/07] **[Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models](https://arxiv.org/abs/2307.14539)**
- [2023/08] **["Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models](https://arxiv.org/abs/2308.03825)**
- [2023/11] **[DeepInception: Hypnotize Large Language Model to Be Jailbreaker](https://arxiv.org/abs/2311.03191)**
- [2024/03] **[Here comes the AI worm: Unleashing zero-click worms that target GenAI-powered applications](https://arxiv.org/abs/2403.02817)**
- [2024/07] **[Breaking agents: Compromising autonomous llm agents through malfunction amplification](https://arxiv.org/abs/2407.20859)**
- [2025/01] **[You Can't Eat Your Cake and Have It Too: The Performance Degradation of LLMs with Jailbreak Defense](https://arxiv.org/abs/2501.12210)**
- [2025/03] **[Multi-agent systems execute arbitrary malicious code](https://arxiv.org/abs/2503.12188)**
