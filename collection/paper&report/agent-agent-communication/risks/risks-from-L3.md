# Risks from L3
**Note**: Due to the forward-looking nature of this paper, the risks we mentioned include both **the proposed ones** and **potential directions that have not been studied**. As a result, some risks may lack a concrete list of papers, we hope our paper can inspire these directions.
## R1: MAS Pollution
- [2024/01] **[Psysafe: A comprehensive framework for psychological-based attack, defense, and evaluation of multi-agent system safety](https://arxiv.org/abs/2401.11880)**
- [2024/07] **[Flooding spread of manipulated knowledge in llm-based multi-agent communities](https://arxiv.org/abs/2407.07791)**
- [2024/08] **[On the resilience of multi-agent systems with malicious agents](https://arxiv.org/abs/2408.00989)**
- [2024/10] **[Prompt infection: Llm-to-llm prompt injection within multi-agent systems](https://arxiv.org/abs/2410.07283)**
- [2025/03] **[Agents Under Siege: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks](https://arxiv.org/abs/2504.00218)**
- [2025/09] **[Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems](https://arxiv.org/abs/2509.21054)**

## R2: Privacy Leakage
- [2025/03] **[Prompt flow integrity to prevent privilege escalation in llm agents](https://arxiv.org/abs/2503.15547)**

## R3: Description Poisoning
- [2025/04] **[Securing genai multi-agent systems against tool squatting: A zero trust registry-based approach](https://arxiv.org/abs/2504.19951)**
- [2025/05] **[Deep Dive: MCP and A2A Attack Vectors for AI Agents](https://www.solo.io/blog/deep-dive-mcp-and-a2a-attack-vectors-for-ai-agents)**

## R4: Centralized Poisoning

## R5: Semantic Rewriting Attack

## R6: Cognitive and Ethical Drift

## R7: Contextual Fragmentation
